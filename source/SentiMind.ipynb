{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiMind, Sentence Sentiment Clasfier\n",
    "### Input and Output\n",
    "The input is a sentence or sentences. The output is a list of 0-26 represent one or more sentiment including admiration, gratitude, surprise, or neutural.\n",
    "\n",
    "### Architecture\n",
    "the model or architecture we plan is \n",
    "1. Tokenizing the input\n",
    "2. use one of the pretrained encoder transformer from hugging face to process the embedding, make it context aware to increase accuracy.\n",
    "3. pass the context aware embedding to multiple layer of CNN, maybe 6 layers, each of a configuration of VGG level. \n",
    "4. connect the output of CNN to flattened and connect to FNN to make final classfication.\n",
    "5. softmax the result, include all class with prob greater than a threshold. For example, if a class is more than 0.3, include as output. anger 0.4, remorese 0.35, rest 0.25, output [anger, remorses]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Tokenizing the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for algebric computations\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Admiration',\n",
      " 1: 'Amusement',\n",
      " 2: 'Anger',\n",
      " 3: 'Annoyance',\n",
      " 4: 'Approval',\n",
      " 5: 'Caring',\n",
      " 6: 'Confusion',\n",
      " 7: 'Curiosity',\n",
      " 8: 'Desire',\n",
      " 9: 'Disappointment',\n",
      " 10: 'Disapproval',\n",
      " 11: 'Disgust',\n",
      " 12: 'Embarrassment',\n",
      " 13: 'Excitement',\n",
      " 14: 'Fear',\n",
      " 15: 'Gratitude',\n",
      " 16: 'Grief',\n",
      " 17: 'Joy',\n",
      " 18: 'Love',\n",
      " 19: 'Nervousness',\n",
      " 20: 'Optimism',\n",
      " 21: 'Pride',\n",
      " 22: 'Realization',\n",
      " 23: 'Relief',\n",
      " 24: 'Remorse',\n",
      " 25: 'Sadness',\n",
      " 26: 'Surprise'}\n"
     ]
    }
   ],
   "source": [
    "id_to_class = {\n",
    "    0: 'Admiration',\n",
    "    1: 'Amusement',\n",
    "    2: 'Anger',\n",
    "    3: 'Annoyance',\n",
    "    4: 'Approval',\n",
    "    5: 'Caring',\n",
    "    6: 'Confusion',\n",
    "    7: 'Curiosity',\n",
    "    8: 'Desire',\n",
    "    9: 'Disappointment',\n",
    "    10: 'Disapproval',\n",
    "    11: 'Disgust',\n",
    "    12: 'Embarrassment',\n",
    "    13: 'Excitement',\n",
    "    14: 'Fear',\n",
    "    15: 'Gratitude',\n",
    "    16: 'Grief',\n",
    "    17: 'Joy',\n",
    "    18: 'Love',\n",
    "    19: 'Nervousness',\n",
    "    20: 'Optimism',\n",
    "    21: 'Pride',\n",
    "    22: 'Realization',\n",
    "    23: 'Relief',\n",
    "    24: 'Remorse',\n",
    "    25: 'Sadness',\n",
    "    26: 'Surprise'\n",
    "    }\n",
    "\n",
    "pprint(id_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model\n",
    "model_name = \"distilbert-base-uncased\"  # Replace with \"albert-base-v2\", \"distilroberta-base\", or \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "embedding_size = model.config.hidden_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text 1: i love this product! something, no matter how long as long as this is\n",
      "Decoded Text 2: the experience was terrible that i don ' t want to come any more and the number is depend on the longest sequence\n",
      "tensor([ 1.5044e-01,  1.8223e-02,  1.3812e-01, -1.4606e-01,  1.0705e-02,\n",
      "        -2.3983e-01,  1.1390e-01,  6.2418e-01, -1.5608e-01, -2.2146e-01,\n",
      "         9.4245e-03, -2.3805e-01,  1.2921e-02,  6.4960e-01,  7.2195e-02,\n",
      "         5.0208e-02, -9.9061e-02,  3.9943e-01,  1.3665e-01,  8.4229e-03,\n",
      "        -1.6087e-01, -2.5076e-01,  4.9469e-02,  1.1748e-01, -1.3496e-03,\n",
      "        -1.1265e-01, -4.6018e-02, -1.4053e-01,  1.2362e-01, -4.3372e-03,\n",
      "         5.1253e-02,  4.6455e-02, -2.1266e-01, -2.0892e-01,  2.5433e-03,\n",
      "        -1.5094e-01,  2.6488e-02, -1.6228e-01, -5.6696e-02, -3.9978e-02,\n",
      "        -1.4650e-01,  1.7114e-02,  1.8113e-01, -1.6018e-01, -2.8664e-01,\n",
      "        -2.7828e-01, -2.4839e+00, -2.5722e-02, -1.5833e-01, -7.7120e-02,\n",
      "         2.9434e-01, -2.7885e-02, -1.7000e-02,  1.3479e-01,  3.8139e-01,\n",
      "         3.3676e-01, -4.2024e-01,  3.3696e-01, -1.3984e-01, -8.6244e-02,\n",
      "         2.4298e-01,  2.8483e-03, -1.8761e-01,  7.1805e-03, -1.3016e-02,\n",
      "         3.4427e-02, -3.6076e-02,  5.2288e-02, -1.3227e-01,  2.9959e-01,\n",
      "        -1.8048e-01, -4.3265e-01,  1.7539e-01, -1.4421e-01, -1.2146e-01,\n",
      "        -1.8015e-01, -1.7989e-01,  1.2288e-01,  7.2132e-02,  2.2904e-01,\n",
      "         8.7138e-02,  4.5562e-01,  3.0450e-01,  2.0978e-01,  7.8242e-02,\n",
      "         2.5022e-01, -2.0589e-01, -1.2829e-01,  1.5282e-01,  4.4647e-01,\n",
      "        -2.4917e-01, -9.9371e-03,  2.8073e-01,  1.4485e-01,  5.8011e-01,\n",
      "        -4.9489e-01, -2.6493e-02, -1.1287e-01,  5.1825e-02,  2.3812e-01,\n",
      "         1.6499e-01, -2.0581e-01, -1.5630e-01, -5.1534e-01,  2.7861e-02,\n",
      "        -1.9509e-02,  1.4226e-01, -2.8673e-01,  2.7204e-01, -2.3298e+00,\n",
      "         2.8057e-01,  1.3688e-01, -1.3620e-01, -2.2167e-01, -2.0431e-01,\n",
      "         3.0461e-01,  3.2855e-01,  8.7708e-02,  1.3133e-01, -6.1014e-02,\n",
      "         2.6020e-03,  1.8049e-01, -2.6557e-02,  1.0196e-02,  6.4921e-02,\n",
      "         2.4462e-01, -1.4875e-01, -3.9159e-02, -6.4786e-02,  4.6683e-02,\n",
      "         2.9027e-01,  4.9389e-01,  4.4299e-02, -2.6940e-01, -8.4272e-02,\n",
      "         2.1312e-01,  5.6986e-02, -2.7944e-01, -2.0251e-01, -1.5611e-01,\n",
      "        -1.9233e-01,  8.2808e-03, -2.9317e+00,  3.5277e-01,  2.3307e-01,\n",
      "         5.3262e-02, -1.5444e-01,  1.6089e-01, -7.1103e-03,  1.2513e-01,\n",
      "         1.8493e-01, -1.9019e-02, -4.0160e-01, -4.3846e-02, -1.2329e-01,\n",
      "        -4.6810e-02, -3.7491e-01,  3.7671e-02,  2.6443e-01,  1.7458e-01,\n",
      "         7.7722e-02, -1.6590e-01,  2.7298e-02, -1.5520e-01, -3.5929e-01,\n",
      "         1.3455e-01,  3.6457e-01,  2.6644e-01,  4.3066e-02, -1.3803e-01,\n",
      "        -4.8806e-02,  1.0340e-01,  3.0999e-01,  3.5395e-02,  1.8182e-01,\n",
      "        -7.5847e-02,  2.5096e-01,  4.2225e-01,  6.9095e-02, -6.5769e-02,\n",
      "        -3.3240e-02,  4.6566e-01,  1.1795e-01,  8.5569e-02,  1.5078e-01,\n",
      "        -5.8011e-02,  2.7532e-01, -1.7914e-01, -1.0598e-01,  1.8576e-01,\n",
      "        -1.9076e-01, -1.7911e-01,  1.3034e-01, -2.1308e-01,  1.5188e-01,\n",
      "        -3.5760e-02,  2.2396e-02, -4.6429e-01,  1.2929e-01,  2.3679e-01,\n",
      "        -2.5655e-01, -2.5350e-02, -1.0714e-01, -8.0130e-02, -1.1292e-01,\n",
      "         3.6295e+00,  2.1813e-01, -1.8154e-02,  1.6061e-01, -2.3096e-02,\n",
      "         4.3069e-02,  1.6681e-01,  2.0384e-02, -2.2128e-01, -2.7154e-02,\n",
      "        -5.0421e-02,  2.1430e-01,  1.1916e-01,  8.3554e-02, -1.0943e-01,\n",
      "         1.2513e-01,  1.5399e-01, -1.6601e-01,  4.1275e-02, -1.3415e-02,\n",
      "         3.9625e-02,  1.0361e-03, -6.4852e-02,  2.2159e-02, -1.2838e+00,\n",
      "        -1.8613e-02, -1.5036e-01, -2.3732e-01,  3.0553e-01, -1.8238e-01,\n",
      "         2.0107e-02,  1.8226e-01,  8.5466e-02,  5.3703e-02,  6.0735e-02,\n",
      "        -7.8671e-02,  3.8064e-01,  2.3215e-01,  2.1898e-01, -4.4306e-01,\n",
      "         4.6805e-01,  2.0830e-01,  1.0902e-01, -2.7572e-02, -1.2214e-01,\n",
      "         1.7766e-01, -8.2514e-02, -6.8355e-02,  1.6665e-02,  2.5236e-02,\n",
      "        -2.0671e-01, -6.4277e-02,  1.7717e-01, -3.2953e-01, -1.8167e-02,\n",
      "        -2.1371e-01, -3.2796e-01,  1.2935e-01,  1.4798e-01, -3.3730e-01,\n",
      "        -1.5331e-01, -5.5852e-02, -2.4494e-01,  1.1660e-01,  1.8885e-01,\n",
      "        -1.9573e-01,  4.4022e-02, -2.6460e-01, -3.3563e+00,  2.5834e-02,\n",
      "        -5.3238e-02,  2.6762e-01,  2.4169e-01, -6.2904e-02,  4.2278e-02,\n",
      "         1.8820e-01,  2.4933e-01, -3.9857e-01,  2.6677e-01,  2.9737e-01,\n",
      "        -4.1389e-02,  2.3383e-01, -3.1276e-01,  1.0495e-01,  1.6660e-01,\n",
      "        -1.4548e-01, -2.6310e-01, -3.2689e-01, -2.1561e-01,  3.3469e-01,\n",
      "        -1.1896e-01,  2.7841e-01, -3.4299e-03, -1.1789e-01, -1.5983e-01,\n",
      "        -1.5165e-01,  2.6229e-02,  1.6262e-02, -2.5973e-03, -8.7219e-02,\n",
      "         1.3906e-01, -8.9810e-02, -3.3068e-01, -3.0389e+00,  6.7402e-03,\n",
      "        -1.4640e-01, -8.4488e-02,  1.7531e-01, -5.5829e-02,  2.9842e-01,\n",
      "        -3.8817e-02, -1.3139e-01, -4.8148e-02,  2.5635e-01,  2.5218e-02,\n",
      "        -1.1873e-03,  1.4252e-01,  2.3369e-01, -3.5530e-02,  2.3444e-01,\n",
      "         6.1858e-02,  1.9711e-01,  1.3346e-01, -3.5972e-03, -2.3799e-01,\n",
      "        -6.0139e-02, -5.8543e-02,  1.7760e-01,  2.6501e-01, -4.9895e-01,\n",
      "        -9.6962e-02, -1.9305e-01,  2.1858e-01, -1.7677e-01, -2.7271e-01,\n",
      "         3.9291e-02, -1.8137e-01, -1.2480e-01, -2.0546e-01,  7.8295e-02,\n",
      "        -1.2959e-01,  2.9542e-01, -3.4266e-02,  1.6240e-01,  5.0661e-01,\n",
      "         4.7069e-02,  2.2326e-01,  4.4524e-01,  6.7547e-02,  1.0455e-01,\n",
      "        -9.2616e-02, -2.0989e-01,  2.2572e-01,  6.1814e-02,  4.9160e-02,\n",
      "         1.3271e+00, -2.4287e-01,  2.1386e-02, -2.1299e-01,  4.4408e-01,\n",
      "         6.9193e-02,  2.2302e-01,  2.0274e-01,  3.5434e-01, -1.1964e-01,\n",
      "         1.6373e-01, -4.7805e-02, -7.8093e-02, -1.4591e-01,  2.0480e-01,\n",
      "        -4.8230e-01, -1.3859e-01,  8.9808e-02,  2.1856e-03,  8.6358e-02,\n",
      "        -1.9674e-01, -8.3329e-01, -3.6289e-01,  1.4769e-01, -8.6351e-02,\n",
      "         1.8266e-01,  8.9300e-02,  1.3748e-01, -3.7533e-01, -1.3589e-01,\n",
      "        -1.8447e-02,  3.7795e-01, -1.8636e-01, -5.8277e-02, -6.9792e-02,\n",
      "        -4.4516e-02, -3.5072e-01,  8.3721e-02,  5.1402e-02,  1.2456e-01,\n",
      "         8.5248e-02,  1.4916e-01,  1.6995e-01,  2.2133e-02,  1.4577e-01,\n",
      "        -6.6223e-01,  1.1321e-01, -2.0090e-01, -2.0343e-02, -8.4153e-02,\n",
      "        -9.1965e-02,  1.5508e-01, -1.7990e-01, -3.0216e-01, -2.4422e-01,\n",
      "         2.9026e-01,  1.2218e-01,  1.8503e-01, -4.2525e-02,  1.5110e-01,\n",
      "        -1.0445e-01,  7.1656e-02,  8.6377e-01, -2.0974e-01,  5.8302e-02,\n",
      "         4.1710e-01, -9.7850e-02,  9.8632e-02,  3.2802e-01,  8.2903e-02,\n",
      "        -2.4456e-01,  7.8216e-02, -8.6830e-02,  1.1826e-02,  4.2336e-02,\n",
      "        -1.3222e-01, -2.1205e-01, -7.0102e-02,  1.3125e-01,  1.2631e-01,\n",
      "        -2.0138e-01, -3.0602e-01, -6.4085e-02, -7.3075e-03, -1.7083e-01,\n",
      "        -8.2872e-02,  7.3355e-02,  6.4105e-03,  3.7700e-01,  3.6462e-01,\n",
      "        -2.1105e-01,  3.1376e-01, -1.7550e-01,  5.7392e-01, -7.2438e-02,\n",
      "         2.2343e-01, -2.0270e-01,  1.4426e-01, -9.9929e-02, -1.3467e-01,\n",
      "         1.0600e-02, -2.3913e-01,  3.2259e-01,  2.0999e-01,  4.8031e-02,\n",
      "         5.6792e-02, -5.0556e-03,  8.3324e-03,  1.5679e-01,  7.6116e-02,\n",
      "        -1.4935e+00,  4.3415e-01,  2.8923e-01,  1.5622e-01, -2.7602e-02,\n",
      "        -2.3105e-02, -1.8834e-01,  4.6046e-01,  1.8910e-01,  1.4329e-01,\n",
      "        -9.0335e-02,  4.7618e-02, -1.2310e-02,  1.5772e-02, -1.0474e-02,\n",
      "        -6.6237e-02,  1.8271e-01, -3.8705e-02,  6.9119e-02, -9.3512e-02,\n",
      "         5.3017e-02,  6.7665e-02, -1.4242e-02, -3.3954e-02, -3.6918e-03,\n",
      "        -1.7579e-01,  6.9404e-02,  2.9035e-01,  4.2659e-03,  2.3635e-01,\n",
      "        -9.0083e-02, -4.1690e-01, -5.6403e-01, -3.0250e-01,  1.1840e-01,\n",
      "        -1.0803e-01,  9.2902e-02, -1.5741e-02,  2.2338e-01,  1.6389e-01,\n",
      "        -5.5412e-01,  1.7088e-01,  8.4855e-02, -9.2439e-02,  4.1899e-01,\n",
      "         2.7340e-01, -3.2486e-02,  2.4068e-01, -5.3586e-02, -2.4022e-01,\n",
      "         1.3123e-01,  8.4987e-02, -1.4594e-01, -1.5190e-01,  8.0932e-02,\n",
      "        -7.2976e-02, -8.0868e-02,  5.1334e-02, -1.7785e-01, -1.1972e-01,\n",
      "         3.1404e-01, -2.1981e-01, -1.1297e-01,  2.0888e-01, -1.8151e-01,\n",
      "        -3.9443e-01, -8.4777e-02, -3.9982e-01,  8.5757e-02,  1.2125e-02,\n",
      "         4.4414e-01,  2.0879e-02, -2.2951e-01,  1.7524e-01, -1.8139e-01,\n",
      "         3.0290e-01, -1.1887e-01,  2.1815e-02,  1.4877e-01, -3.2347e-01,\n",
      "        -5.1926e-02, -2.7297e-01, -3.4514e-01,  1.4155e-01, -3.8857e-02,\n",
      "         2.7689e-01, -1.9868e-01, -4.0154e-02, -4.7732e-02,  1.0128e-01,\n",
      "        -3.9792e-01, -1.8690e-01,  1.1382e-01, -9.7505e-02,  4.8286e-03,\n",
      "         1.7983e-01,  8.2506e-02, -2.2438e-01, -1.1401e-01,  6.6507e-02,\n",
      "        -2.3237e-01,  3.2775e-01,  5.1012e-01,  3.2195e-01, -1.3157e-01,\n",
      "         1.6555e-01,  2.0435e-01,  1.3526e-01, -1.8837e-01,  1.1180e-01,\n",
      "        -2.3716e-02,  1.2184e-01,  1.5723e-01,  1.9195e-01,  3.5114e-02,\n",
      "        -1.4478e-01, -2.0695e-02, -3.9342e-01,  1.9003e+00,  4.0021e-01,\n",
      "         5.3759e-02, -4.7345e-02,  3.3330e-01, -6.7450e-03, -1.2807e-01,\n",
      "         1.5890e-01,  2.9955e-02,  4.5620e-01, -2.1486e-01,  1.7964e-01,\n",
      "        -2.6397e-02,  1.1358e-02,  3.7312e-01,  9.5339e-02,  1.6126e-01,\n",
      "        -3.2716e-01, -4.1649e-01, -3.1677e-02, -4.1226e-01,  2.2612e-01,\n",
      "         3.7890e-01, -7.6119e-02, -1.2369e-01,  2.7090e-01,  1.9142e-01,\n",
      "         1.4538e-02, -1.1602e-01,  2.6767e-01,  3.0051e-02, -6.3247e-02,\n",
      "         4.0238e-02,  2.8449e-01, -3.1401e-01,  1.2647e-01,  2.5081e-01,\n",
      "        -3.3343e-01, -1.0227e-02,  1.8231e-01, -4.3186e-02, -4.8666e-01,\n",
      "         3.3607e-01, -4.8669e-02, -1.0369e-01,  4.3241e-01, -1.8252e-01,\n",
      "        -1.0269e-01,  3.1084e-01,  2.6507e-01, -2.2677e-01,  7.4110e-02,\n",
      "        -2.0410e-01,  2.6852e-01, -5.2987e-02, -2.6936e-01, -1.4023e-01,\n",
      "        -9.0300e-02, -1.6791e-01,  3.6620e-01, -7.1676e-03, -2.6858e-03,\n",
      "         2.7855e-01,  1.5836e-01,  2.5367e-01,  1.8449e-01, -2.6594e-01,\n",
      "         1.8114e-01,  1.8425e-01, -2.4044e-01,  1.2077e-02,  1.9936e-01,\n",
      "         3.3460e-01,  1.8410e-02,  2.9646e-01,  7.0249e-02,  6.3656e-01,\n",
      "         7.5737e-02,  9.5739e-02, -2.5153e+00,  1.0069e-01,  3.3012e-02,\n",
      "         2.4053e-01, -7.5415e-02,  3.0917e-01,  3.1764e-01, -5.2871e-02,\n",
      "         8.8329e-02, -2.8376e-01,  2.9893e-01,  3.1018e-01,  3.0631e-01,\n",
      "         1.0415e-01,  2.6003e-01, -3.2702e-04,  2.5214e-01, -7.5264e-02,\n",
      "        -2.3378e-01, -1.1856e-01, -3.5123e-02,  4.4216e-02,  1.0562e-01,\n",
      "        -1.8473e-01, -1.3018e-01,  8.7058e-02, -8.3855e-02, -1.6311e-01,\n",
      "        -1.1841e-01,  1.1235e-01, -2.1635e-01,  3.5437e-01, -2.5594e-01,\n",
      "        -1.8624e-01,  1.1170e-01, -4.4229e-02, -2.7890e-01, -9.1621e-02,\n",
      "         9.2210e-02,  8.3196e-02, -8.8759e-02,  5.2058e-01, -7.3973e-02,\n",
      "         1.1947e-01,  5.0699e-02,  6.5249e-02,  3.3760e-01, -1.9885e-01,\n",
      "         1.6720e-01, -3.4889e-01, -2.1191e-02, -1.9892e-02,  2.4343e-01,\n",
      "         7.1965e-02,  8.1205e-02, -1.2378e-02,  1.0035e-03,  1.5849e-01,\n",
      "         6.1041e-02, -1.7345e-01, -1.5293e-01,  2.8496e-02,  2.2304e-02,\n",
      "        -1.7582e-01,  3.2122e-01, -3.0251e-01, -1.3560e-01,  1.7783e-01,\n",
      "        -5.9724e-02, -2.2506e-01, -1.4072e-01, -1.1318e-01,  1.4273e-01,\n",
      "         2.2428e-01,  1.9946e-01, -9.0083e-02,  2.9101e-01,  2.5129e-01,\n",
      "        -7.0372e-02,  9.7723e-02, -2.0676e-01, -2.1474e-02,  2.1952e-01,\n",
      "        -7.5382e-02, -6.8772e-02, -6.8816e+00, -1.4855e-01, -1.6015e-01,\n",
      "        -1.0033e-01, -1.0853e-01, -1.1951e-01, -1.7382e-01, -2.4538e-01,\n",
      "         2.9060e-01, -1.9679e-01,  2.0860e-01, -3.4831e-04, -1.7938e-02,\n",
      "        -7.0177e-02,  2.9922e-01,  2.0241e-01])\n",
      "Last Hidden State Shape: torch.Size([2, 25, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example input text\n",
    "texts = [\"I love this product! something, no matter how long as long as this is \", \"The experience was terrible that I don't want to come any more and the number is depend on the longest sequence\"]\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in inputs.input_ids]\n",
    "for i, text in enumerate(decoded_texts):\n",
    "    print(f\"Decoded Text {i+1}: {text}\")\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Output: last_hidden_state (embeddings of shape [batch_size, seq_len, hidden_dim])\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "print(last_hidden_state[0][0])\n",
    "print(f\"Last Hidden State Shape: {last_hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: pass the context aware embedding to multiple layer of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(torch.nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        \"\"\"\n",
    "        k1 k2: conv layer size, conv layer size: k1 x embedding_size\n",
    "        n1 n2: channel #\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        k1 = 2\n",
    "        k2 = 4\n",
    "        channel_size1 = 10\n",
    "        channel_size2 = 20\n",
    "\n",
    "        kernal_size1 = (k1, embedding_size)\n",
    "        kernal_size2 = (k2, embedding_size)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(1, channel_size1, kernal_size1, bias=False)\n",
    "        self.conv2 = torch.nn.Conv2d(1, channel_size2, kernal_size2, bias=False)\n",
    "\n",
    "        self.output_model = torch.nn.Linear(channel_size1 + channel_size2 , 26)\n",
    "\n",
    "\n",
    "    def forward(self, embedded_text):\n",
    "        # embedded_text: [2, 7, 768] batch 2, all sentences max length 8, embedding size 768\n",
    "        sentence_emb = embedded_text.unsqueeze(1)  # for conv2d()\n",
    "\n",
    "        conv1_out = self.conv1(sentence_emb)\n",
    "        conv2_out = self.conv2(sentence_emb)\n",
    "\n",
    "        conv1_out = torch.nn.functional.relu(conv1_out)\n",
    "        conv2_out = torch.nn.functional.relu(conv2_out)\n",
    "\n",
    "        conv1_out = conv1_out.squeeze(3)\n",
    "        conv2_out = conv2_out.squeeze(3)\n",
    "\n",
    "        # maxpool\n",
    "        conv1_out = torch.nn.functional.max_pool1d(conv1_out, conv1_out.size(2)).squeeze(2)  # [batch_size, n1]\n",
    "        conv2_out = torch.nn.functional.max_pool1d(conv2_out, conv2_out.size(2)).squeeze(2)  # [batch_size, n2]\n",
    "\n",
    "\n",
    "        after_maxpool = torch.cat((conv1_out, conv2_out), 1)\n",
    "        # return torch.sigmoid(self.output_model(after_maxpool))  # apply sigmoid\n",
    "        distribution = self.output_model(after_maxpool)\n",
    "        return F.softmax(distribution, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = CNN_Model(embedding_size)\n",
    "\n",
    "output = CNN(last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0289, 0.0212, 0.0436, 0.0337, 0.0380, 0.0396, 0.0384, 0.0260, 0.0361,\n",
      "         0.0290, 0.0407, 0.0430, 0.0395, 0.0414, 0.0395, 0.0469, 0.0516, 0.0552,\n",
      "         0.0408, 0.0379, 0.0384, 0.0574, 0.0349, 0.0452, 0.0269, 0.0260],\n",
      "        [0.0288, 0.0225, 0.0462, 0.0371, 0.0390, 0.0378, 0.0387, 0.0285, 0.0368,\n",
      "         0.0286, 0.0399, 0.0433, 0.0385, 0.0428, 0.0434, 0.0492, 0.0454, 0.0501,\n",
      "         0.0402, 0.0355, 0.0389, 0.0548, 0.0335, 0.0437, 0.0295, 0.0273]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
